{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "84Q8JfvaeZZ6"
   },
   "source": [
    "## Linear Classifier in TensorFlow \n",
    "Using Low Level API in Eager Execution mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHpCNRv1OB5-"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mjtb-EMcm5K0"
   },
   "outputs": [],
   "source": [
    "#Enable Eager Execution if using tensflow version < 2.0\n",
    "#From tensorflow v2.0 onwards, Eager Execution will be enabled by default\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DxJDmJqqOB6K",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhllFLyKOB6N"
   },
   "outputs": [],
   "source": [
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KiObW4V4SIOz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B4yQKMiJOB6R"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('prices.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgkX6SEqOB6W"
   },
   "source": [
    "### Check all columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7K8pWsNQOB6X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 851264 entries, 0 to 851263\n",
      "Data columns (total 7 columns):\n",
      "date      851264 non-null object\n",
      "symbol    851264 non-null object\n",
      "open      851264 non-null float64\n",
      "close     851264 non-null float64\n",
      "low       851264 non-null float64\n",
      "high      851264 non-null float64\n",
      "volume    851264 non-null float64\n",
      "dtypes: float64(5), object(2)\n",
      "memory usage: 45.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dU6X7MpOB6c"
   },
   "source": [
    "### Drop columns `date` and  `symbol`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lh_6spSKOB6e"
   },
   "outputs": [],
   "source": [
    "data.drop(columns=[\"date\",\"symbol\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xlwbUgTwOB6i",
    "outputId": "56bad82a-f271-415a-e0d6-cbe1c4290743"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high     volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163600.0\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386400.0\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489500.0\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006300.0\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408600.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3DBv3WWYOB6q"
   },
   "source": [
    "### Consider only first 1000 rows in the dataset for building feature set and target set\n",
    "Target 'Volume' has very high values. Divide 'Volume' by 1000,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z_hG9rGBOB6s"
   },
   "outputs": [],
   "source": [
    "data_1000=data.head(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1000.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1000[\"volume\"]=data_1000[\"volume\"]/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.430000</td>\n",
       "      <td>125.839996</td>\n",
       "      <td>122.309998</td>\n",
       "      <td>126.250000</td>\n",
       "      <td>2163.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>125.239998</td>\n",
       "      <td>119.980003</td>\n",
       "      <td>119.940002</td>\n",
       "      <td>125.540001</td>\n",
       "      <td>2386.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116.379997</td>\n",
       "      <td>114.949997</td>\n",
       "      <td>114.930000</td>\n",
       "      <td>119.739998</td>\n",
       "      <td>2489.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>115.480003</td>\n",
       "      <td>116.620003</td>\n",
       "      <td>113.500000</td>\n",
       "      <td>117.440002</td>\n",
       "      <td>2006.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117.010002</td>\n",
       "      <td>114.970001</td>\n",
       "      <td>114.089996</td>\n",
       "      <td>117.330002</td>\n",
       "      <td>1408.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         open       close         low        high  volume\n",
       "0  123.430000  125.839996  122.309998  126.250000  2163.6\n",
       "1  125.239998  119.980003  119.940002  125.540001  2386.4\n",
       "2  116.379997  114.949997  114.930000  119.739998  2489.5\n",
       "3  115.480003  116.620003  113.500000  117.440002  2006.3\n",
       "4  117.010002  114.970001  114.089996  117.330002  1408.6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1000.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3UaApqYOB6x"
   },
   "source": [
    "### Divide the data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4LE4U8lTdQJq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_1000.drop(columns=\"volume\")\n",
    "Y=data_1000[\"volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYK-aUuLbrz2"
   },
   "source": [
    "#### Convert Training and Test Data to numpy float32 arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ao-S0tQGcncz"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train_arr=np.asarray(X_train)\n",
    "#X_test_arr=np.asarray(X_test)\n",
    "X_train_arr=np.float32(X_train)\n",
    "X_test_arr=np.float32(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im1ZegbDdKgv"
   },
   "source": [
    "### Normalize the data\n",
    "You can use Normalizer from sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2EkKAy7fOB6y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "C:\\Users\\kalya\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "X_train_std=scale(X_train_arr)\n",
    "X_test_std=scale(X_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6471594 , -0.65192837, -0.6477564 , -0.6535503 ],\n",
       "       [-0.8951312 , -0.89168936, -0.89293355, -0.89432305],\n",
       "       [-0.8009922 , -0.7985978 , -0.79978985, -0.80114216],\n",
       "       ...,\n",
       "       [ 9.375546  ,  9.332643  ,  9.377511  ,  9.330112  ],\n",
       "       [ 0.9817958 ,  0.94452477,  0.94749826,  0.9782794 ],\n",
       "       [ 0.77863616,  0.7824517 ,  0.7736976 ,  0.8027333 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6vE4eYCOB62",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building the Model in tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "297_qja4OB7A",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1.Define Weights and Bias, use tf.zeros to initialize weights and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input features\n",
    "x = tf.placeholder(shape=[None,4],dtype=tf.float32, name='x-input')\n",
    "\n",
    "#Normalize the data\n",
    "#x_n = tf.nn.l2_normalize(x,1)\n",
    "\n",
    "#Actual Prices\n",
    "y_ = tf.placeholder(shape=[None],dtype=tf.float32, name='y-input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L205qPeQOB7B"
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal(shape=[4,1]), name=\"Weights\")\n",
    "b = tf.Variable(tf.zeros(shape=[1]),name=\"Bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Weights_4:0' shape=(4, 1) dtype=float32_ref>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HgtWA-UIOB7F",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2.Define a function to calculate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JveGlx25OB7H"
   },
   "outputs": [],
   "source": [
    "y = tf.add(tf.matmul(x,W),b,name='output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL1hIwf_OB7M",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "3.Loss (Cost) Function [Mean square error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VSWPiGXOB7P"
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y-y_),name='Loss')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzG85FUlOB7U",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "4.Function to train the Model\n",
    "\n",
    "1.   Record all the mathematical steps to calculate Loss\n",
    "2.   Calculate Gradients of Loss w.r.t weights and bias\n",
    "3.   Update Weights and Bias based on gradients and learning rate to minimize loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cj802w-3OB7X"
   },
   "outputs": [],
   "source": [
    "train_op = tf.train.GradientDescentOptimizer(0.3).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xSypb_u8OB7e",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the model for 100 epochs \n",
    "1. Observe the training loss at every iteration\n",
    "2. Observe Test loss at every 5th iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DVvgj7eQOB7f"
   },
   "outputs": [],
   "source": [
    "W_arr=[]\n",
    "b_arr=[]\n",
    "#Lets start graph Execution\n",
    "sess = tf.Session()\n",
    "\n",
    "# variables need to be initialized before we can use them\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#how many times data need to be shown to model\n",
    "training_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at step:  0  is  225879650.0\n",
      "Training loss at step:  5  is  195406770.0\n",
      "Training loss at step:  10  is  195403550.0\n",
      "Training loss at step:  15  is  195403680.0\n",
      "Training loss at step:  20  is  195408100.0\n",
      "Training loss at step:  25  is  195536290.0\n",
      "Training loss at step:  30  is  199240800.0\n",
      "Training loss at step:  35  is  306292860.0\n",
      "Training loss at step:  40  is  3399875800.0\n",
      "Training loss at step:  45  is  92797950000.0\n",
      "Training loss at step:  50  is  2676219400000.0\n",
      "Training loss at step:  55  is  77331420000000.0\n",
      "Training loss at step:  60  is  2234719100000000.0\n",
      "Training loss at step:  65  is  6.4578858e+16\n",
      "Training loss at step:  70  is  1.8661563e+18\n",
      "Training loss at step:  75  is  5.392762e+19\n",
      "Training loss at step:  80  is  1.5583935e+21\n",
      "Training loss at step:  85  is  4.5034024e+22\n",
      "Training loss at step:  90  is  1.3014132e+24\n",
      "Training loss at step:  95  is  3.7608463e+25\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "            \n",
    "    #Calculate train_op and loss\n",
    "    _, train_loss = sess.run([train_op,loss],feed_dict={x:X_train_std, y_:y_train})\n",
    "    W_arr.append(sess.run(W))\n",
    "    b_arr.append(sess.run(b))\n",
    "    if epoch % 5 == 0:\n",
    "        print ('Training loss at step: ', epoch, ' is ', train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing necessary values to be used outside the Session \n",
    "#training_cost = sess.run([train_op,loss], feed_dict ={X: X_train_std, Y: y_train}) \n",
    "weight = sess.run(W) \n",
    "bias = sess.run(b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOL2ncA1OB7q"
   },
   "source": [
    "### Get the shapes and values of W and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZGvtyTeuOB7r"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.407263  ],\n",
       "        [-1.0919313 ],\n",
       "        [ 0.73944336],\n",
       "        [ 0.6458762 ]], dtype=float32), array([[-0.33905852],\n",
       "        [-1.0236696 ],\n",
       "        [ 0.80767876],\n",
       "        [ 0.71411276]], dtype=float32), array([[-0.43451083],\n",
       "        [-1.1190796 ],\n",
       "        [ 0.71223617],\n",
       "        [ 0.6186856 ]], dtype=float32), array([[-0.30092365],\n",
       "        [-0.98544866],\n",
       "        [ 0.8458213 ],\n",
       "        [ 0.75228995]], dtype=float32), array([[-0.48797256],\n",
       "        [-1.1724561 ],\n",
       "        [ 0.6587634 ],\n",
       "        [ 0.56525093]], dtype=float32), array([[-0.22615722],\n",
       "        [-0.9105984 ],\n",
       "        [ 0.9205704 ],\n",
       "        [ 0.8270819 ]], dtype=float32), array([[-0.5926807 ],\n",
       "        [-1.2770809 ],\n",
       "        [ 0.55403614],\n",
       "        [ 0.4605662 ]], dtype=float32), array([[-0.07963157],\n",
       "        [-0.76398885],\n",
       "        [ 1.0670769 ],\n",
       "        [ 0.97363305]], dtype=float32), array([[-0.7978616 ],\n",
       "        [-1.4821789 ],\n",
       "        [ 0.34883487],\n",
       "        [ 0.25540668]], dtype=float32), array([[ 0.20753837],\n",
       "        [-0.47673488],\n",
       "        [ 1.3542278 ],\n",
       "        [ 1.2608306 ]], dtype=float32), array([[-1.1998907 ],\n",
       "        [-1.8841256 ],\n",
       "        [-0.05321515],\n",
       "        [-0.14660263]], dtype=float32), array([[0.77027595],\n",
       "        [0.08608794],\n",
       "        [1.9169466 ],\n",
       "        [1.8235992 ]], dtype=float32), array([[-1.9877173 ],\n",
       "        [-2.671871  ],\n",
       "        [-0.84106517],\n",
       "        [-0.934415  ]], dtype=float32), array([[1.8730568],\n",
       "        [1.1889548],\n",
       "        [3.0197098],\n",
       "        [2.9264174]], dtype=float32), array([[-3.531527 ],\n",
       "        [-4.2156   ],\n",
       "        [-2.384899 ],\n",
       "        [-2.4782176]], dtype=float32), array([[4.0341578],\n",
       "        [3.3501444],\n",
       "        [5.1807947],\n",
       "        [5.087565 ]], dtype=float32), array([[-6.556821],\n",
       "        [-7.240815],\n",
       "        [-5.410225],\n",
       "        [-5.503526]], dtype=float32), array([[8.269106 ],\n",
       "        [7.5851955],\n",
       "        [9.415734 ],\n",
       "        [9.322588 ]], dtype=float32), array([[-12.485235],\n",
       "        [-13.169157],\n",
       "        [-11.338668],\n",
       "        [-11.431982]], dtype=float32), array([[16.56805 ],\n",
       "        [15.884232],\n",
       "        [17.714691],\n",
       "        [17.621643]], dtype=float32), array([[-24.10271 ],\n",
       "        [-24.78658 ],\n",
       "        [-22.956188],\n",
       "        [-23.049568]], dtype=float32), array([[32.83092 ],\n",
       "        [32.147217],\n",
       "        [33.977554],\n",
       "        [33.88475 ]], dtype=float32), array([[-46.868603],\n",
       "        [-47.55246 ],\n",
       "        [-45.722237],\n",
       "        [-45.81569 ]], dtype=float32), array([[64.70015],\n",
       "        [64.0166 ],\n",
       "        [65.84677],\n",
       "        [65.75433]], dtype=float32), array([[-91.48131],\n",
       "        [-92.16516],\n",
       "        [-90.33508],\n",
       "        [-90.42886]], dtype=float32), array([[127.15187],\n",
       "        [126.46858],\n",
       "        [128.29875],\n",
       "        [128.20682]], dtype=float32), array([[-178.90553],\n",
       "        [-179.58943],\n",
       "        [-177.75945],\n",
       "        [-177.85406]], dtype=float32), array([[249.53401],\n",
       "        [248.85103],\n",
       "        [250.68108],\n",
       "        [250.5903 ]], dtype=float32), array([[-350.22394],\n",
       "        [-350.90826],\n",
       "        [-349.0785 ],\n",
       "        [-349.17447]], dtype=float32), array([[489.3568 ],\n",
       "        [488.67444],\n",
       "        [490.50427],\n",
       "        [490.41586]], dtype=float32), array([[-685.94446],\n",
       "        [-686.629  ],\n",
       "        [-684.7997 ],\n",
       "        [-684.8983 ]], dtype=float32), array([[959.32056],\n",
       "        [958.63965],\n",
       "        [960.4691 ],\n",
       "        [960.38525]], dtype=float32), array([[-1343.831 ],\n",
       "        [-1344.5164],\n",
       "        [-1342.6886],\n",
       "        [-1342.7915]], dtype=float32), array([[1880.273 ],\n",
       "        [1879.5962],\n",
       "        [1881.4232],\n",
       "        [1881.3496]], dtype=float32), array([[-2633.041 ],\n",
       "        [-2633.732 ],\n",
       "        [-2631.9038],\n",
       "        [-2632.0176]], dtype=float32), array([[3684.998 ],\n",
       "        [3684.3218],\n",
       "        [3686.1533],\n",
       "        [3686.0942]], dtype=float32), array([[-5159.416 ],\n",
       "        [-5160.111 ],\n",
       "        [-5158.283 ],\n",
       "        [-5158.4185]], dtype=float32), array([[7221.58  ],\n",
       "        [7220.9155],\n",
       "        [7222.749 ],\n",
       "        [7222.724 ]], dtype=float32), array([[-10110.162],\n",
       "        [-10110.873],\n",
       "        [-10109.038],\n",
       "        [-10109.223]], dtype=float32), array([[14151.969],\n",
       "        [14151.32 ],\n",
       "        [14153.157],\n",
       "        [14153.184]], dtype=float32), array([[-19811.777],\n",
       "        [-19812.508],\n",
       "        [-19810.68 ],\n",
       "        [-19810.953]], dtype=float32), array([[27732.945],\n",
       "        [27732.324],\n",
       "        [27734.156],\n",
       "        [27734.305]], dtype=float32), array([[-38823.305],\n",
       "        [-38824.113],\n",
       "        [-38822.297],\n",
       "        [-38822.742]], dtype=float32), array([[54346.586],\n",
       "        [54345.98 ],\n",
       "        [54347.85 ],\n",
       "        [54348.25 ]], dtype=float32), array([[-76078.85],\n",
       "        [-76079.72],\n",
       "        [-76077.92],\n",
       "        [-76078.63]], dtype=float32), array([[106499.32],\n",
       "        [106498.91],\n",
       "        [106500.7 ],\n",
       "        [106501.62]], dtype=float32), array([[-149085.81],\n",
       "        [-149086.83],\n",
       "        [-149085.19],\n",
       "        [-149086.5 ]], dtype=float32), array([[208699.5 ],\n",
       "        [208699.27],\n",
       "        [208701.06],\n",
       "        [208702.88]], dtype=float32), array([[-292152.16],\n",
       "        [-292153.5 ],\n",
       "        [-292151.88],\n",
       "        [-292154.56]], dtype=float32), array([[408973.22],\n",
       "        [408973.7 ],\n",
       "        [408975.44],\n",
       "        [408978.94]], dtype=float32), array([[-572509.1],\n",
       "        [-572510.6],\n",
       "        [-572509.3],\n",
       "        [-572514.5]], dtype=float32), array([[801434.5 ],\n",
       "        [801436.75],\n",
       "        [801438.56],\n",
       "        [801445.75]], dtype=float32), array([[-1121903.6],\n",
       "        [-1121906.2],\n",
       "        [-1121904. ],\n",
       "        [-1121914.8]], dtype=float32), array([[1570511.9],\n",
       "        [1570515.2],\n",
       "        [1570517.5],\n",
       "        [1570530.8]], dtype=float32), array([[-2198507.5],\n",
       "        [-2198512.8],\n",
       "        [-2198512.5],\n",
       "        [-2198531. ]], dtype=float32), array([[3077615.5],\n",
       "        [3077622.2],\n",
       "        [3077624.5],\n",
       "        [3077651.5]], dtype=float32), array([[-4308253.5],\n",
       "        [-4308265. ],\n",
       "        [-4308262. ],\n",
       "        [-4308303. ]], dtype=float32), array([[6030987.5],\n",
       "        [6031000. ],\n",
       "        [6031006. ],\n",
       "        [6031053. ]], dtype=float32), array([[-8442582.],\n",
       "        [-8442609.],\n",
       "        [-8442599.],\n",
       "        [-8442685.]], dtype=float32), array([[11818486.],\n",
       "        [11818503.],\n",
       "        [11818519.],\n",
       "        [11818611.]], dtype=float32), array([[-16544300.],\n",
       "        [-16544343.],\n",
       "        [-16544343.],\n",
       "        [-16544487.]], dtype=float32), array([[23159852.],\n",
       "        [23159896.],\n",
       "        [23159912.],\n",
       "        [23160116.]], dtype=float32), array([[-32420748.],\n",
       "        [-32420848.],\n",
       "        [-32420852.],\n",
       "        [-32421128.]], dtype=float32), array([[45384780.],\n",
       "        [45384824.],\n",
       "        [45384860.],\n",
       "        [45385240.]], dtype=float32), array([[-63532348.],\n",
       "        [-63532616.],\n",
       "        [-63532532.],\n",
       "        [-63533160.]], dtype=float32), array([[88936720.],\n",
       "        [88936824.],\n",
       "        [88936960.],\n",
       "        [88937690.]], dtype=float32), array([[-1.2449966e+08],\n",
       "        [-1.2449996e+08],\n",
       "        [-1.2449998e+08],\n",
       "        [-1.2450108e+08]], dtype=float32), array([[1.7428235e+08],\n",
       "        [1.7428278e+08],\n",
       "        [1.7428286e+08],\n",
       "        [1.7428445e+08]], dtype=float32), array([[-2.4397128e+08],\n",
       "        [-2.4397184e+08],\n",
       "        [-2.4397195e+08],\n",
       "        [-2.4397405e+08]], dtype=float32), array([[3.4152614e+08],\n",
       "        [3.4152704e+08],\n",
       "        [3.4152704e+08],\n",
       "        [3.4153014e+08]], dtype=float32), array([[-4.7809146e+08],\n",
       "        [-4.7809274e+08],\n",
       "        [-4.7809299e+08],\n",
       "        [-4.7809706e+08]], dtype=float32), array([[6.6926496e+08],\n",
       "        [6.6926611e+08],\n",
       "        [6.6926650e+08],\n",
       "        [6.6927219e+08]], dtype=float32), array([[-9.3687930e+08],\n",
       "        [-9.3688186e+08],\n",
       "        [-9.3688173e+08],\n",
       "        [-9.3689037e+08]], dtype=float32), array([[1.3115075e+09],\n",
       "        [1.3115103e+09],\n",
       "        [1.3115116e+09],\n",
       "        [1.3115223e+09]], dtype=float32), array([[-1.8359306e+09],\n",
       "        [-1.8359350e+09],\n",
       "        [-1.8359357e+09],\n",
       "        [-1.8359514e+09]], dtype=float32), array([[2.5700680e+09],\n",
       "        [2.5700733e+09],\n",
       "        [2.5700751e+09],\n",
       "        [2.5700995e+09]], dtype=float32), array([[-3.5977539e+09],\n",
       "        [-3.5977633e+09],\n",
       "        [-3.5977636e+09],\n",
       "        [-3.5977935e+09]], dtype=float32), array([[5.036359e+09],\n",
       "        [5.036371e+09],\n",
       "        [5.036371e+09],\n",
       "        [5.036417e+09]], dtype=float32), array([[-7.0502385e+09],\n",
       "        [-7.0502584e+09],\n",
       "        [-7.0502615e+09],\n",
       "        [-7.0503199e+09]], dtype=float32), array([[9.869357e+09],\n",
       "        [9.869379e+09],\n",
       "        [9.869379e+09],\n",
       "        [9.869466e+09]], dtype=float32), array([[-1.3815771e+10],\n",
       "        [-1.3815801e+10],\n",
       "        [-1.3815807e+10],\n",
       "        [-1.3815931e+10]], dtype=float32), array([[1.9340304e+10],\n",
       "        [1.9340337e+10],\n",
       "        [1.9340354e+10],\n",
       "        [1.9340526e+10]], dtype=float32), array([[-2.7073696e+10],\n",
       "        [-2.7073794e+10],\n",
       "        [-2.7073790e+10],\n",
       "        [-2.7074036e+10]], dtype=float32), array([[3.7899674e+10],\n",
       "        [3.7899723e+10],\n",
       "        [3.7899739e+10],\n",
       "        [3.7900075e+10]], dtype=float32), array([[-5.3054300e+10],\n",
       "        [-5.3054448e+10],\n",
       "        [-5.3054456e+10],\n",
       "        [-5.3054915e+10]], dtype=float32), array([[7.426915e+10],\n",
       "        [7.426932e+10],\n",
       "        [7.426937e+10],\n",
       "        [7.426999e+10]], dtype=float32), array([[-1.03966999e+11],\n",
       "        [-1.03967285e+11],\n",
       "        [-1.03967269e+11],\n",
       "        [-1.03968219e+11]], dtype=float32), array([[1.4554020e+11],\n",
       "        [1.4554051e+11],\n",
       "        [1.4554061e+11],\n",
       "        [1.4554186e+11]], dtype=float32), array([[-2.0373742e+11],\n",
       "        [-2.0373786e+11],\n",
       "        [-2.0373799e+11],\n",
       "        [-2.0373979e+11]], dtype=float32), array([[2.8520573e+11],\n",
       "        [2.8520645e+11],\n",
       "        [2.8520648e+11],\n",
       "        [2.8520894e+11]], dtype=float32), array([[-3.9925082e+11],\n",
       "        [-3.9925187e+11],\n",
       "        [-3.9925177e+11],\n",
       "        [-3.9925573e+11]], dtype=float32), array([[5.5889835e+11],\n",
       "        [5.5889940e+11],\n",
       "        [5.5889979e+11],\n",
       "        [5.5890438e+11]], dtype=float32), array([[-7.8238161e+11],\n",
       "        [-7.8238384e+11],\n",
       "        [-7.8238423e+11],\n",
       "        [-7.8239131e+11]], dtype=float32), array([[1.09523003e+12],\n",
       "        [1.09523226e+12],\n",
       "        [1.09523226e+12],\n",
       "        [1.09524156e+12]], dtype=float32), array([[-1.5331804e+12],\n",
       "        [-1.5331837e+12],\n",
       "        [-1.5331847e+12],\n",
       "        [-1.5331979e+12]], dtype=float32), array([[2.1462379e+12],\n",
       "        [2.1462449e+12],\n",
       "        [2.1462436e+12],\n",
       "        [2.1462636e+12]], dtype=float32), array([[-3.0044460e+12],\n",
       "        [-3.0044507e+12],\n",
       "        [-3.0044560e+12],\n",
       "        [-3.0044790e+12]], dtype=float32), array([[4.2058279e+12],\n",
       "        [4.2058425e+12],\n",
       "        [4.2058404e+12],\n",
       "        [4.2058798e+12]], dtype=float32), array([[-5.8876137e+12],\n",
       "        [-5.8876263e+12],\n",
       "        [-5.8876284e+12],\n",
       "        [-5.8876751e+12]], dtype=float32), array([[8.241886e+12],\n",
       "        [8.241905e+12],\n",
       "        [8.241903e+12],\n",
       "        [8.241984e+12]], dtype=float32)]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sess.run(W)\n",
    "W_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3312.3079], dtype=float32),\n",
       " array([4637.2305], dtype=float32),\n",
       " array([5167.1997], dtype=float32),\n",
       " array([5379.1875], dtype=float32),\n",
       " array([5463.9824], dtype=float32),\n",
       " array([5497.9004], dtype=float32),\n",
       " array([5511.468], dtype=float32),\n",
       " array([5516.8945], dtype=float32),\n",
       " array([5519.0654], dtype=float32),\n",
       " array([5519.9336], dtype=float32),\n",
       " array([5520.2812], dtype=float32),\n",
       " array([5520.42], dtype=float32),\n",
       " array([5520.4756], dtype=float32),\n",
       " array([5520.498], dtype=float32),\n",
       " array([5520.507], dtype=float32),\n",
       " array([5520.5103], dtype=float32),\n",
       " array([5520.5117], dtype=float32),\n",
       " array([5520.512], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.5127], dtype=float32),\n",
       " array([5520.513], dtype=float32),\n",
       " array([5520.513], dtype=float32),\n",
       " array([5520.513], dtype=float32),\n",
       " array([5520.5117], dtype=float32),\n",
       " array([5520.51], dtype=float32),\n",
       " array([5520.512], dtype=float32),\n",
       " array([5520.5083], dtype=float32),\n",
       " array([5520.5103], dtype=float32),\n",
       " array([5520.523], dtype=float32),\n",
       " array([5520.5254], dtype=float32),\n",
       " array([5520.5215], dtype=float32),\n",
       " array([5520.54], dtype=float32),\n",
       " array([5520.531], dtype=float32),\n",
       " array([5520.6235], dtype=float32),\n",
       " array([5520.462], dtype=float32),\n",
       " array([5520.5674], dtype=float32),\n",
       " array([5520.5063], dtype=float32),\n",
       " array([5520.15], dtype=float32),\n",
       " array([5521.3125], dtype=float32),\n",
       " array([5520.0376], dtype=float32),\n",
       " array([5517.15], dtype=float32),\n",
       " array([5518.5186], dtype=float32),\n",
       " array([5523.15], dtype=float32),\n",
       " array([5516.55], dtype=float32),\n",
       " array([5520.412], dtype=float32),\n",
       " array([5535.787], dtype=float32),\n",
       " array([5509.162], dtype=float32),\n",
       " array([5512.6123], dtype=float32),\n",
       " array([5483.9624], dtype=float32),\n",
       " array([5649.8623], dtype=float32),\n",
       " array([5462.3623], dtype=float32),\n",
       " array([5418.5625], dtype=float32),\n",
       " array([5419.1626], dtype=float32),\n",
       " array([5468.363], dtype=float32),\n",
       " array([6272.363], dtype=float32),\n",
       " array([6889.163], dtype=float32),\n",
       " array([7249.163], dtype=float32),\n",
       " array([6932.3633], dtype=float32),\n",
       " array([5175.5635], dtype=float32),\n",
       " array([3812.3633], dtype=float32),\n",
       " array([2535.5632], dtype=float32),\n",
       " array([7777.163], dtype=float32),\n",
       " array([13287.563], dtype=float32),\n",
       " array([26573.965], dtype=float32),\n",
       " array([7220.3633], dtype=float32),\n",
       " array([13287.563], dtype=float32),\n",
       " array([48308.363], dtype=float32),\n",
       " array([33255.562], dtype=float32),\n",
       " array([86861.97], dtype=float32),\n",
       " array([15284.367], dtype=float32),\n",
       " array([127105.17], dtype=float32),\n",
       " array([-51070.83], dtype=float32),\n",
       " array([-124798.83], dtype=float32),\n",
       " array([-37554.023], dtype=float32),\n",
       " array([-418482.03], dtype=float32),\n",
       " array([-575768.44], dtype=float32),\n",
       " array([-912459.6], dtype=float32),\n",
       " array([296679.62], dtype=float32),\n",
       " array([1299380.5], dtype=float32),\n",
       " array([1441921.2], dtype=float32),\n",
       " array([3584948.5], dtype=float32)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vhDtOv5UOB7x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost = 5.54597e+26 Weight = [[8.241886e+12]\n",
      " [8.241905e+12]\n",
      " [8.241903e+12]\n",
      " [8.241984e+12]] bias = [3584948.5] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the predictions \n",
    "predictions = np.matmul(X_test_std[1],weight) + bias \n",
    "print(\"Training cost =\", train_loss, \"Weight =\", weight, \"bias =\", bias, '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Model Prediction on 1st Examples in Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKGvUWahcihp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.524173e+12], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YJRBuqXhOB7_"
   },
   "source": [
    "## Classification using tf.Keras\n",
    "\n",
    "In this exercise, we will build a Deep Neural Network using tf.Keras. We will use Iris Dataset for this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sb7Epo0VOB58"
   },
   "source": [
    "### Load tensorflow if not done already"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0g6lorycihf"
   },
   "source": [
    "### Load the given Iris data using pandas (Iris.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xFvb5sRcihg"
   },
   "outputs": [],
   "source": [
    "df_iris=pd.read_csv(\"Iris (2).csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SAB--Qdwcihm"
   },
   "source": [
    "### Target set has different categories. So, Label encode them. And convert into one-hot vectors using get_dummies in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJr5dYnocihm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris[\"Species\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb=LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris['Species']= lb.fit_transform(df_iris['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm  Species\n",
       "0   1            5.1           3.5            1.4           0.2        0\n",
       "1   2            4.9           3.0            1.4           0.2        0\n",
       "2   3            4.7           3.2            1.3           0.2        0\n",
       "3   4            4.6           3.1            1.5           0.2        0\n",
       "4   5            5.0           3.6            1.4           0.2        0"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris_dum=pd.get_dummies(df_iris,columns=[\"Species\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D95nY5ILcihj"
   },
   "source": [
    "### Splitting the data into feature set and target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RyMQoLMucihj"
   },
   "outputs": [],
   "source": [
    "X=df_iris_dum.drop(columns=[\"Id\",\"Species_0\",\"Species_1\",\"Species_2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=df_iris_dum.iloc[:,5:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species_0</th>\n",
       "      <th>Species_1</th>\n",
       "      <th>Species_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Species_0  Species_1  Species_2\n",
       "0          1          0          0\n",
       "1          1          0          0\n",
       "2          1          0          0\n",
       "3          1          0          0\n",
       "4          1          0          0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERq9GOKKciho"
   },
   "source": [
    "### Divide the dataset into Training and test (70:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gKGvUWahcihp"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b22qpC5xcihr"
   },
   "source": [
    "###  Building Model in tf.keras\n",
    "\n",
    "Build a Linear Classifier model  <br>\n",
    "1.  Use Dense Layer  with input shape of 4 (according to the feature set) and number of outputs set to 3<br> \n",
    "2. Apply Softmax on Dense Layer outputs <br>\n",
    "3. Use SGD as Optimizer\n",
    "4. Use categorical_crossentropy as loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hov_UFnUciht"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=4, activation='relu'))\n",
    "#model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5FdzqIKcihw"
   },
   "source": [
    "### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qLEdHPscihx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/150\n",
      "105/105 [==============================] - 0s 4ms/step - loss: 0.8175 - acc: 0.5714 - val_loss: 0.5444 - val_acc: 0.6000\n",
      "Epoch 2/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.5185 - acc: 0.7524 - val_loss: 0.4924 - val_acc: 0.6222\n",
      "Epoch 3/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4407 - acc: 0.7619 - val_loss: 0.4042 - val_acc: 0.9333\n",
      "Epoch 4/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4118 - acc: 0.8000 - val_loss: 0.3976 - val_acc: 0.7556\n",
      "Epoch 5/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3641 - acc: 0.8762 - val_loss: 0.4086 - val_acc: 0.7111\n",
      "Epoch 6/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3453 - acc: 0.8571 - val_loss: 0.3345 - val_acc: 0.8444\n",
      "Epoch 7/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3099 - acc: 0.9143 - val_loss: 0.2743 - val_acc: 0.9556\n",
      "Epoch 8/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2949 - acc: 0.8667 - val_loss: 0.3752 - val_acc: 0.7778\n",
      "Epoch 9/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2578 - acc: 0.9333 - val_loss: 0.3823 - val_acc: 0.7556\n",
      "Epoch 10/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2430 - acc: 0.9333 - val_loss: 0.2161 - val_acc: 1.0000\n",
      "Epoch 11/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2225 - acc: 0.8952 - val_loss: 0.3521 - val_acc: 0.7556\n",
      "Epoch 12/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2527 - acc: 0.9429 - val_loss: 0.2120 - val_acc: 0.9333\n",
      "Epoch 13/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2302 - acc: 0.9333 - val_loss: 0.1943 - val_acc: 0.9556\n",
      "Epoch 14/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2114 - acc: 0.9048 - val_loss: 0.2274 - val_acc: 0.9111\n",
      "Epoch 15/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2203 - acc: 0.9143 - val_loss: 0.2718 - val_acc: 0.8444\n",
      "Epoch 16/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1905 - acc: 0.9333 - val_loss: 0.1525 - val_acc: 1.0000\n",
      "Epoch 17/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1708 - acc: 0.9238 - val_loss: 0.1556 - val_acc: 0.9778\n",
      "Epoch 18/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.2024 - acc: 0.9238 - val_loss: 0.1394 - val_acc: 1.0000\n",
      "Epoch 19/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1898 - acc: 0.9333 - val_loss: 0.1385 - val_acc: 0.9778\n",
      "Epoch 20/150\n",
      "105/105 [==============================] - 0s 998us/step - loss: 0.1696 - acc: 0.9524 - val_loss: 0.1467 - val_acc: 0.9556\n",
      "Epoch 21/150\n",
      "105/105 [==============================] - 0s 952us/step - loss: 0.1910 - acc: 0.9333 - val_loss: 0.1320 - val_acc: 0.9778\n",
      "Epoch 22/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1541 - acc: 0.9238 - val_loss: 0.1214 - val_acc: 0.9556\n",
      "Epoch 23/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1914 - acc: 0.9238 - val_loss: 0.1743 - val_acc: 0.9111\n",
      "Epoch 24/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1624 - acc: 0.9143 - val_loss: 0.1794 - val_acc: 0.9333\n",
      "Epoch 25/150\n",
      "105/105 [==============================] - 0s 979us/step - loss: 0.1457 - acc: 0.9429 - val_loss: 0.1914 - val_acc: 0.9111\n",
      "Epoch 26/150\n",
      "105/105 [==============================] - 0s 945us/step - loss: 0.1634 - acc: 0.9429 - val_loss: 0.2543 - val_acc: 0.8444\n",
      "Epoch 27/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1411 - acc: 0.9524 - val_loss: 0.1084 - val_acc: 0.9556\n",
      "Epoch 28/150\n",
      "105/105 [==============================] - 0s 954us/step - loss: 0.1610 - acc: 0.9238 - val_loss: 0.2435 - val_acc: 0.8667\n",
      "Epoch 29/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1602 - acc: 0.9238 - val_loss: 0.1039 - val_acc: 0.9778\n",
      "Epoch 30/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1306 - acc: 0.9524 - val_loss: 0.0968 - val_acc: 1.0000\n",
      "Epoch 31/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1729 - acc: 0.9333 - val_loss: 0.2814 - val_acc: 0.8444\n",
      "Epoch 32/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1625 - acc: 0.9333 - val_loss: 0.2347 - val_acc: 0.8889\n",
      "Epoch 33/150\n",
      "105/105 [==============================] - 0s 962us/step - loss: 0.1524 - acc: 0.9429 - val_loss: 0.1753 - val_acc: 0.9111\n",
      "Epoch 34/150\n",
      "105/105 [==============================] - 0s 931us/step - loss: 0.1585 - acc: 0.9429 - val_loss: 0.1146 - val_acc: 0.9778\n",
      "Epoch 35/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1516 - acc: 0.9619 - val_loss: 0.1134 - val_acc: 0.9778\n",
      "Epoch 36/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1667 - acc: 0.9238 - val_loss: 0.0905 - val_acc: 1.0000\n",
      "Epoch 37/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1647 - acc: 0.9429 - val_loss: 0.0924 - val_acc: 0.9778\n",
      "Epoch 38/150\n",
      "105/105 [==============================] - 0s 916us/step - loss: 0.1487 - acc: 0.9429 - val_loss: 0.1194 - val_acc: 0.9556\n",
      "Epoch 39/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1577 - acc: 0.9524 - val_loss: 0.2097 - val_acc: 0.9111\n",
      "Epoch 40/150\n",
      "105/105 [==============================] - 0s 920us/step - loss: 0.1069 - acc: 0.9619 - val_loss: 0.2154 - val_acc: 0.9111\n",
      "Epoch 41/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1487 - acc: 0.9429 - val_loss: 0.0854 - val_acc: 0.9778\n",
      "Epoch 42/150\n",
      "105/105 [==============================] - 0s 910us/step - loss: 0.1019 - acc: 0.9810 - val_loss: 0.2708 - val_acc: 0.8667\n",
      "Epoch 43/150\n",
      "105/105 [==============================] - 0s 941us/step - loss: 0.1677 - acc: 0.9333 - val_loss: 0.0846 - val_acc: 0.9778\n",
      "Epoch 44/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1587 - acc: 0.9333 - val_loss: 0.0796 - val_acc: 1.0000\n",
      "Epoch 45/150\n",
      "105/105 [==============================] - 0s 944us/step - loss: 0.0969 - acc: 0.9714 - val_loss: 0.1104 - val_acc: 0.9556\n",
      "Epoch 46/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1494 - acc: 0.9619 - val_loss: 0.1142 - val_acc: 0.9556\n",
      "Epoch 47/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1107 - acc: 0.9524 - val_loss: 0.2057 - val_acc: 0.9111\n",
      "Epoch 48/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1677 - acc: 0.9333 - val_loss: 0.0767 - val_acc: 1.0000\n",
      "Epoch 49/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1441 - acc: 0.9524 - val_loss: 0.1462 - val_acc: 0.9111\n",
      "Epoch 50/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1079 - acc: 0.9619 - val_loss: 0.6155 - val_acc: 0.7556\n",
      "Epoch 51/150\n",
      "105/105 [==============================] - 0s 992us/step - loss: 0.1759 - acc: 0.9429 - val_loss: 0.0862 - val_acc: 0.9778\n",
      "Epoch 52/150\n",
      "105/105 [==============================] - 0s 910us/step - loss: 0.1209 - acc: 0.9714 - val_loss: 0.2289 - val_acc: 0.8889\n",
      "Epoch 53/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1440 - acc: 0.9429 - val_loss: 0.1934 - val_acc: 0.9111\n",
      "Epoch 54/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1204 - acc: 0.9714 - val_loss: 0.2016 - val_acc: 0.9111\n",
      "Epoch 55/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1307 - acc: 0.9524 - val_loss: 0.0713 - val_acc: 1.0000\n",
      "Epoch 56/150\n",
      "105/105 [==============================] - 0s 873us/step - loss: 0.1641 - acc: 0.9333 - val_loss: 0.0879 - val_acc: 0.9556\n",
      "Epoch 57/150\n",
      "105/105 [==============================] - 0s 915us/step - loss: 0.1410 - acc: 0.9238 - val_loss: 0.0879 - val_acc: 0.9778\n",
      "Epoch 58/150\n",
      "105/105 [==============================] - 0s 983us/step - loss: 0.1505 - acc: 0.9429 - val_loss: 0.2332 - val_acc: 0.8889\n",
      "Epoch 59/150\n",
      "105/105 [==============================] - 0s 915us/step - loss: 0.1265 - acc: 0.9619 - val_loss: 0.1018 - val_acc: 0.9556\n",
      "Epoch 60/150\n",
      "105/105 [==============================] - 0s 903us/step - loss: 0.1357 - acc: 0.9429 - val_loss: 0.1978 - val_acc: 0.9111\n",
      "Epoch 61/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 951us/step - loss: 0.1344 - acc: 0.9619 - val_loss: 0.1460 - val_acc: 0.9333\n",
      "Epoch 62/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1298 - acc: 0.9429 - val_loss: 0.4074 - val_acc: 0.8000\n",
      "Epoch 63/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1271 - acc: 0.9333 - val_loss: 0.0698 - val_acc: 1.0000\n",
      "Epoch 64/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1195 - acc: 0.9524 - val_loss: 0.1656 - val_acc: 0.9111\n",
      "Epoch 65/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1192 - acc: 0.9619 - val_loss: 0.1457 - val_acc: 0.9333\n",
      "Epoch 66/150\n",
      "105/105 [==============================] - 0s 878us/step - loss: 0.1472 - acc: 0.9524 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "Epoch 67/150\n",
      "105/105 [==============================] - 0s 920us/step - loss: 0.1188 - acc: 0.9524 - val_loss: 0.0712 - val_acc: 0.9778\n",
      "Epoch 68/150\n",
      "105/105 [==============================] - 0s 957us/step - loss: 0.1258 - acc: 0.9524 - val_loss: 0.2321 - val_acc: 0.8889\n",
      "Epoch 69/150\n",
      "105/105 [==============================] - 0s 930us/step - loss: 0.1315 - acc: 0.9429 - val_loss: 0.1141 - val_acc: 0.9556\n",
      "Epoch 70/150\n",
      "105/105 [==============================] - 0s 846us/step - loss: 0.1324 - acc: 0.9524 - val_loss: 0.0885 - val_acc: 0.9556\n",
      "Epoch 71/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1614 - acc: 0.9333 - val_loss: 0.0828 - val_acc: 0.9778\n",
      "Epoch 72/150\n",
      "105/105 [==============================] - 0s 915us/step - loss: 0.1216 - acc: 0.9524 - val_loss: 0.1505 - val_acc: 0.9333\n",
      "Epoch 73/150\n",
      "105/105 [==============================] - 0s 867us/step - loss: 0.1718 - acc: 0.9333 - val_loss: 0.0663 - val_acc: 1.0000\n",
      "Epoch 74/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.0979 - acc: 0.9810 - val_loss: 0.2214 - val_acc: 0.8889\n",
      "Epoch 75/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1472 - acc: 0.9429 - val_loss: 0.0662 - val_acc: 1.0000\n",
      "Epoch 76/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1462 - acc: 0.9524 - val_loss: 0.0660 - val_acc: 1.0000\n",
      "Epoch 77/150\n",
      "105/105 [==============================] - 0s 944us/step - loss: 0.1212 - acc: 0.9524 - val_loss: 0.0642 - val_acc: 1.0000\n",
      "Epoch 78/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1138 - acc: 0.9524 - val_loss: 0.1407 - val_acc: 0.9333\n",
      "Epoch 79/150\n",
      "105/105 [==============================] - 0s 908us/step - loss: 0.1155 - acc: 0.9619 - val_loss: 0.0735 - val_acc: 0.9778\n",
      "Epoch 80/150\n",
      "105/105 [==============================] - 0s 912us/step - loss: 0.1084 - acc: 0.9524 - val_loss: 0.1553 - val_acc: 0.9111\n",
      "Epoch 81/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1306 - acc: 0.9524 - val_loss: 0.1331 - val_acc: 0.9333\n",
      "Epoch 82/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1330 - acc: 0.9619 - val_loss: 0.1122 - val_acc: 0.9556\n",
      "Epoch 83/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1273 - acc: 0.9524 - val_loss: 0.0683 - val_acc: 0.9778\n",
      "Epoch 84/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1275 - acc: 0.9714 - val_loss: 0.0638 - val_acc: 1.0000\n",
      "Epoch 85/150\n",
      "105/105 [==============================] - 0s 878us/step - loss: 0.1113 - acc: 0.9524 - val_loss: 0.1389 - val_acc: 0.9333\n",
      "Epoch 86/150\n",
      "105/105 [==============================] - 0s 961us/step - loss: 0.1092 - acc: 0.9524 - val_loss: 0.0638 - val_acc: 1.0000\n",
      "Epoch 87/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1217 - acc: 0.9524 - val_loss: 0.1287 - val_acc: 0.9333\n",
      "Epoch 88/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1380 - acc: 0.9429 - val_loss: 0.1260 - val_acc: 0.9333\n",
      "Epoch 89/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1138 - acc: 0.9619 - val_loss: 0.1237 - val_acc: 0.9333\n",
      "Epoch 90/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1128 - acc: 0.9619 - val_loss: 0.0718 - val_acc: 0.9778\n",
      "Epoch 91/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1120 - acc: 0.9429 - val_loss: 0.1379 - val_acc: 0.9333\n",
      "Epoch 92/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1370 - acc: 0.9524 - val_loss: 0.0689 - val_acc: 0.9778\n",
      "Epoch 93/150\n",
      "105/105 [==============================] - 0s 934us/step - loss: 0.1209 - acc: 0.9619 - val_loss: 0.0979 - val_acc: 0.9556\n",
      "Epoch 94/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1046 - acc: 0.9429 - val_loss: 0.1308 - val_acc: 0.9333\n",
      "Epoch 95/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1168 - acc: 0.9524 - val_loss: 0.1166 - val_acc: 0.9333\n",
      "Epoch 96/150\n",
      "105/105 [==============================] - 0s 917us/step - loss: 0.1177 - acc: 0.9714 - val_loss: 0.1163 - val_acc: 0.9333\n",
      "Epoch 97/150\n",
      "105/105 [==============================] - 0s 917us/step - loss: 0.1183 - acc: 0.9524 - val_loss: 0.1605 - val_acc: 0.9111\n",
      "Epoch 98/150\n",
      "105/105 [==============================] - 0s 896us/step - loss: 0.0964 - acc: 0.9714 - val_loss: 0.2042 - val_acc: 0.9111\n",
      "Epoch 99/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1508 - acc: 0.9238 - val_loss: 0.1195 - val_acc: 0.9333\n",
      "Epoch 100/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1061 - acc: 0.9714 - val_loss: 0.1952 - val_acc: 0.9333\n",
      "Epoch 101/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1288 - acc: 0.9619 - val_loss: 0.1166 - val_acc: 0.9333\n",
      "Epoch 102/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1243 - acc: 0.9524 - val_loss: 0.1206 - val_acc: 0.9333\n",
      "Epoch 103/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1157 - acc: 0.9714 - val_loss: 0.1195 - val_acc: 0.9333\n",
      "Epoch 104/150\n",
      "105/105 [==============================] - 0s 954us/step - loss: 0.1305 - acc: 0.9714 - val_loss: 0.1120 - val_acc: 0.9556\n",
      "Epoch 105/150\n",
      "105/105 [==============================] - 0s 957us/step - loss: 0.1375 - acc: 0.9429 - val_loss: 0.1515 - val_acc: 0.9333\n",
      "Epoch 106/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1454 - acc: 0.9333 - val_loss: 0.0753 - val_acc: 0.9778\n",
      "Epoch 107/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1242 - acc: 0.9429 - val_loss: 0.1148 - val_acc: 0.9556\n",
      "Epoch 108/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1246 - acc: 0.9524 - val_loss: 0.0681 - val_acc: 0.9778\n",
      "Epoch 109/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1136 - acc: 0.9619 - val_loss: 0.0657 - val_acc: 0.9778\n",
      "Epoch 110/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1339 - acc: 0.9333 - val_loss: 0.1510 - val_acc: 0.9333\n",
      "Epoch 111/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1192 - acc: 0.9333 - val_loss: 0.1177 - val_acc: 0.9333\n",
      "Epoch 112/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1288 - acc: 0.9429 - val_loss: 0.0598 - val_acc: 1.0000\n",
      "Epoch 113/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1328 - acc: 0.9429 - val_loss: 0.1337 - val_acc: 0.9333\n",
      "Epoch 114/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0978 - acc: 0.9619 - val_loss: 0.0875 - val_acc: 0.9778\n",
      "Epoch 115/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1042 - acc: 0.9524 - val_loss: 0.1019 - val_acc: 0.9556\n",
      "Epoch 116/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1425 - acc: 0.9524 - val_loss: 0.0819 - val_acc: 0.9778\n",
      "Epoch 117/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1042 - acc: 0.9619 - val_loss: 0.0991 - val_acc: 0.9556\n",
      "Epoch 118/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1232 - acc: 0.9524 - val_loss: 0.1379 - val_acc: 0.9333\n",
      "Epoch 119/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1162 - acc: 0.9524 - val_loss: 0.0569 - val_acc: 1.0000\n",
      "Epoch 120/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1300 - acc: 0.9429 - val_loss: 0.0678 - val_acc: 0.9778\n",
      "Epoch 121/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 0s 989us/step - loss: 0.1174 - acc: 0.9619 - val_loss: 0.0611 - val_acc: 0.9778\n",
      "Epoch 122/150\n",
      "105/105 [==============================] - 0s 840us/step - loss: 0.1268 - acc: 0.9524 - val_loss: 0.1044 - val_acc: 0.9556\n",
      "Epoch 123/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1108 - acc: 0.9619 - val_loss: 0.1354 - val_acc: 0.9333\n",
      "Epoch 124/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1135 - acc: 0.9619 - val_loss: 0.1577 - val_acc: 0.9333\n",
      "Epoch 125/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1332 - acc: 0.9524 - val_loss: 0.0587 - val_acc: 1.0000\n",
      "Epoch 126/150\n",
      "105/105 [==============================] - 0s 908us/step - loss: 0.1006 - acc: 0.9524 - val_loss: 0.1464 - val_acc: 0.9333\n",
      "Epoch 127/150\n",
      "105/105 [==============================] - 0s 909us/step - loss: 0.1089 - acc: 0.9429 - val_loss: 0.0787 - val_acc: 0.9778\n",
      "Epoch 128/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1302 - acc: 0.9524 - val_loss: 0.1056 - val_acc: 0.9556\n",
      "Epoch 129/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1291 - acc: 0.9619 - val_loss: 0.0772 - val_acc: 0.9778\n",
      "Epoch 130/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1023 - acc: 0.9429 - val_loss: 0.0866 - val_acc: 0.9778\n",
      "Epoch 131/150\n",
      "105/105 [==============================] - 0s 915us/step - loss: 0.1066 - acc: 0.9429 - val_loss: 0.0830 - val_acc: 0.9778\n",
      "Epoch 132/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1081 - acc: 0.9810 - val_loss: 0.0543 - val_acc: 1.0000\n",
      "Epoch 133/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1376 - acc: 0.9333 - val_loss: 0.0534 - val_acc: 1.0000\n",
      "Epoch 134/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.0932 - acc: 0.9429 - val_loss: 0.0890 - val_acc: 0.9778\n",
      "Epoch 135/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1384 - acc: 0.9429 - val_loss: 0.0594 - val_acc: 0.9778\n",
      "Epoch 136/150\n",
      "105/105 [==============================] - 0s 913us/step - loss: 0.1160 - acc: 0.9524 - val_loss: 0.1278 - val_acc: 0.9333\n",
      "Epoch 137/150\n",
      "105/105 [==============================] - 0s 912us/step - loss: 0.1129 - acc: 0.9524 - val_loss: 0.0533 - val_acc: 1.0000\n",
      "Epoch 138/150\n",
      "105/105 [==============================] - 0s 954us/step - loss: 0.0880 - acc: 0.9810 - val_loss: 0.0913 - val_acc: 0.9778\n",
      "Epoch 139/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.0905 - acc: 0.9524 - val_loss: 0.4061 - val_acc: 0.8222\n",
      "Epoch 140/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1309 - acc: 0.9429 - val_loss: 0.1362 - val_acc: 0.9333\n",
      "Epoch 141/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1157 - acc: 0.9619 - val_loss: 0.0548 - val_acc: 1.0000\n",
      "Epoch 142/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1342 - acc: 0.9429 - val_loss: 0.0845 - val_acc: 0.9778\n",
      "Epoch 143/150\n",
      "105/105 [==============================] - 0s 930us/step - loss: 0.1135 - acc: 0.9524 - val_loss: 0.0907 - val_acc: 0.9556\n",
      "Epoch 144/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1101 - acc: 0.9714 - val_loss: 0.0744 - val_acc: 0.9778\n",
      "Epoch 145/150\n",
      "105/105 [==============================] - 0s 919us/step - loss: 0.0999 - acc: 0.9714 - val_loss: 0.0838 - val_acc: 0.9778\n",
      "Epoch 146/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1362 - acc: 0.9524 - val_loss: 0.0773 - val_acc: 0.9778\n",
      "Epoch 147/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1234 - acc: 0.9524 - val_loss: 0.0628 - val_acc: 0.9778\n",
      "Epoch 148/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1029 - acc: 0.9619 - val_loss: 0.0925 - val_acc: 0.9556\n",
      "Epoch 149/150\n",
      "105/105 [==============================] - 0s 992us/step - loss: 0.1109 - acc: 0.9524 - val_loss: 0.0931 - val_acc: 0.9556\n",
      "Epoch 150/150\n",
      "105/105 [==============================] - 0s 951us/step - loss: 0.1339 - acc: 0.9524 - val_loss: 0.0583 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2387abee898>"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y-SgSSdRcih5"
   },
   "source": [
    "### Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBgKZkhkcih6"
   },
   "outputs": [],
   "source": [
    "y_predict=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-239-5e7cd011bf5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_test' is not defined"
     ]
    }
   ],
   "source": [
    "confusion_matrix(Y_test,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P32ASP1Vjt0a"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n8rd0jjAjyTR"
   },
   "outputs": [],
   "source": [
    "model.save('iris.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XiipRpe7rbVh"
   },
   "source": [
    "### Build and Train a Deep Neural network with 2 hidden layer  - Optional - For Practice\n",
    "\n",
    "Does it perform better than Linear Classifier? What could be the reason for difference in performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v5Du3lubr4sA"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=4, activation='relu'))\n",
    "model.add(Dense(10, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 105 samples, validate on 45 samples\n",
      "Epoch 1/150\n",
      "105/105 [==============================] - 0s 5ms/step - loss: 0.7296 - acc: 0.6857 - val_loss: 0.5469 - val_acc: 0.6000\n",
      "Epoch 2/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.4632 - acc: 0.7524 - val_loss: 0.4158 - val_acc: 0.9556\n",
      "Epoch 3/150\n",
      "105/105 [==============================] - 0s 994us/step - loss: 0.4018 - acc: 0.8095 - val_loss: 0.3491 - val_acc: 0.8000\n",
      "Epoch 4/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3451 - acc: 0.8381 - val_loss: 0.3149 - val_acc: 0.8889\n",
      "Epoch 5/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3291 - acc: 0.8571 - val_loss: 0.2117 - val_acc: 1.0000\n",
      "Epoch 6/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3049 - acc: 0.8381 - val_loss: 0.4017 - val_acc: 0.7556\n",
      "Epoch 7/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3179 - acc: 0.8667 - val_loss: 0.3946 - val_acc: 0.7556\n",
      "Epoch 8/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2881 - acc: 0.8571 - val_loss: 0.7262 - val_acc: 0.6222\n",
      "Epoch 9/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.3118 - acc: 0.8857 - val_loss: 0.3062 - val_acc: 0.8222\n",
      "Epoch 10/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2411 - acc: 0.9143 - val_loss: 0.2230 - val_acc: 0.9111\n",
      "Epoch 11/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2724 - acc: 0.9048 - val_loss: 0.5872 - val_acc: 0.6889\n",
      "Epoch 12/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1889 - acc: 0.9429 - val_loss: 0.2805 - val_acc: 0.8444\n",
      "Epoch 13/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2655 - acc: 0.8762 - val_loss: 0.2655 - val_acc: 0.8444\n",
      "Epoch 14/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2353 - acc: 0.8952 - val_loss: 0.1307 - val_acc: 0.9778\n",
      "Epoch 15/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2170 - acc: 0.9048 - val_loss: 0.9618 - val_acc: 0.6222\n",
      "Epoch 16/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2148 - acc: 0.8857 - val_loss: 0.4934 - val_acc: 0.7556\n",
      "Epoch 17/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1867 - acc: 0.9429 - val_loss: 0.1004 - val_acc: 0.9778\n",
      "Epoch 18/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2101 - acc: 0.9333 - val_loss: 0.1147 - val_acc: 0.9778\n",
      "Epoch 19/150\n",
      "105/105 [==============================] - 0s 915us/step - loss: 0.2215 - acc: 0.9143 - val_loss: 0.1813 - val_acc: 0.9556\n",
      "Epoch 20/150\n",
      "105/105 [==============================] - 0s 941us/step - loss: 0.1511 - acc: 0.9333 - val_loss: 0.1801 - val_acc: 0.9333\n",
      "Epoch 21/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2066 - acc: 0.9143 - val_loss: 0.1183 - val_acc: 0.9778\n",
      "Epoch 22/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1630 - acc: 0.9238 - val_loss: 0.2651 - val_acc: 0.8889\n",
      "Epoch 23/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1605 - acc: 0.9333 - val_loss: 0.1779 - val_acc: 0.9333\n",
      "Epoch 24/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2104 - acc: 0.8952 - val_loss: 0.0816 - val_acc: 0.9778\n",
      "Epoch 25/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2579 - acc: 0.9143 - val_loss: 0.1409 - val_acc: 0.9556\n",
      "Epoch 26/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1425 - acc: 0.9429 - val_loss: 0.0787 - val_acc: 1.0000\n",
      "Epoch 27/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1610 - acc: 0.9429 - val_loss: 1.0654 - val_acc: 0.6222\n",
      "Epoch 28/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1391 - acc: 0.9714 - val_loss: 0.0594 - val_acc: 1.0000\n",
      "Epoch 29/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1688 - acc: 0.9429 - val_loss: 1.0444 - val_acc: 0.7111\n",
      "Epoch 30/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.2536 - acc: 0.9048 - val_loss: 0.6919 - val_acc: 0.7556\n",
      "Epoch 31/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1606 - acc: 0.9333 - val_loss: 0.0597 - val_acc: 1.0000\n",
      "Epoch 32/150\n",
      "105/105 [==============================] - 0s 992us/step - loss: 0.1519 - acc: 0.9619 - val_loss: 0.0717 - val_acc: 1.0000\n",
      "Epoch 33/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1795 - acc: 0.9333 - val_loss: 0.2306 - val_acc: 0.8889\n",
      "Epoch 34/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1587 - acc: 0.9429 - val_loss: 0.2650 - val_acc: 0.8889\n",
      "Epoch 35/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1330 - acc: 0.9333 - val_loss: 0.1705 - val_acc: 0.9111\n",
      "Epoch 36/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1645 - acc: 0.9333 - val_loss: 0.1107 - val_acc: 0.9333\n",
      "Epoch 37/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1617 - acc: 0.9524 - val_loss: 0.1876 - val_acc: 0.9111\n",
      "Epoch 38/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1619 - acc: 0.9524 - val_loss: 0.0652 - val_acc: 1.0000\n",
      "Epoch 39/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1889 - acc: 0.9238 - val_loss: 0.0811 - val_acc: 1.0000\n",
      "Epoch 40/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1219 - acc: 0.9619 - val_loss: 0.0747 - val_acc: 0.9778\n",
      "Epoch 41/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1556 - acc: 0.9524 - val_loss: 0.0877 - val_acc: 0.9778\n",
      "Epoch 42/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1457 - acc: 0.9429 - val_loss: 0.0753 - val_acc: 0.9778\n",
      "Epoch 43/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1396 - acc: 0.9333 - val_loss: 0.0920 - val_acc: 0.9556\n",
      "Epoch 44/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1212 - acc: 0.9524 - val_loss: 0.1418 - val_acc: 0.9333\n",
      "Epoch 45/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1718 - acc: 0.9333 - val_loss: 0.0850 - val_acc: 1.0000\n",
      "Epoch 46/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1136 - acc: 0.9714 - val_loss: 0.2185 - val_acc: 0.9111\n",
      "Epoch 47/150\n",
      "105/105 [==============================] - 0s 954us/step - loss: 0.1241 - acc: 0.9429 - val_loss: 0.2138 - val_acc: 0.8889\n",
      "Epoch 48/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1547 - acc: 0.9238 - val_loss: 0.0628 - val_acc: 0.9778\n",
      "Epoch 49/150\n",
      "105/105 [==============================] - 0s 961us/step - loss: 0.1617 - acc: 0.9524 - val_loss: 0.0675 - val_acc: 1.0000\n",
      "Epoch 50/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1370 - acc: 0.9333 - val_loss: 0.1252 - val_acc: 0.9333\n",
      "Epoch 51/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1542 - acc: 0.9333 - val_loss: 0.0729 - val_acc: 1.0000\n",
      "Epoch 52/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1216 - acc: 0.9524 - val_loss: 0.0730 - val_acc: 0.9778\n",
      "Epoch 53/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1147 - acc: 0.9524 - val_loss: 0.0927 - val_acc: 0.9556\n",
      "Epoch 54/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1687 - acc: 0.9429 - val_loss: 0.0896 - val_acc: 0.9556\n",
      "Epoch 55/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1704 - acc: 0.9429 - val_loss: 0.1721 - val_acc: 0.9333\n",
      "Epoch 56/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1452 - acc: 0.9429 - val_loss: 0.1033 - val_acc: 0.9556\n",
      "Epoch 57/150\n",
      "105/105 [==============================] - 0s 993us/step - loss: 0.0821 - acc: 0.9524 - val_loss: 0.1019 - val_acc: 0.9333\n",
      "Epoch 58/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1785 - acc: 0.9238 - val_loss: 0.1776 - val_acc: 0.9111\n",
      "Epoch 59/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1507 - acc: 0.9333 - val_loss: 0.1512 - val_acc: 0.9333\n",
      "Epoch 60/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1202 - acc: 0.9333 - val_loss: 0.1578 - val_acc: 0.9111\n",
      "Epoch 61/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1518 - acc: 0.9333 - val_loss: 0.5174 - val_acc: 0.7556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1447 - acc: 0.9524 - val_loss: 0.0594 - val_acc: 1.0000\n",
      "Epoch 63/150\n",
      "105/105 [==============================] - 0s 915us/step - loss: 0.1144 - acc: 0.9714 - val_loss: 0.1322 - val_acc: 0.9333\n",
      "Epoch 64/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0618 - acc: 0.9619 - val_loss: 0.0452 - val_acc: 1.0000\n",
      "Epoch 65/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1295 - acc: 0.9619 - val_loss: 0.0649 - val_acc: 0.9778\n",
      "Epoch 66/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1311 - acc: 0.9333 - val_loss: 0.0559 - val_acc: 1.0000\n",
      "Epoch 67/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0982 - acc: 0.9524 - val_loss: 0.0751 - val_acc: 0.9778\n",
      "Epoch 68/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1465 - acc: 0.9619 - val_loss: 0.1062 - val_acc: 0.9333\n",
      "Epoch 69/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1116 - acc: 0.9619 - val_loss: 0.1527 - val_acc: 0.9333\n",
      "Epoch 70/150\n",
      "105/105 [==============================] - 0s 995us/step - loss: 0.1263 - acc: 0.9524 - val_loss: 0.1827 - val_acc: 0.9111\n",
      "Epoch 71/150\n",
      "105/105 [==============================] - 0s 933us/step - loss: 0.1352 - acc: 0.9333 - val_loss: 0.0666 - val_acc: 1.0000\n",
      "Epoch 72/150\n",
      "105/105 [==============================] - 0s 970us/step - loss: 0.1103 - acc: 0.9714 - val_loss: 0.0599 - val_acc: 1.0000\n",
      "Epoch 73/150\n",
      "105/105 [==============================] - 0s 944us/step - loss: 0.1432 - acc: 0.9333 - val_loss: 0.0789 - val_acc: 0.9778\n",
      "Epoch 74/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0882 - acc: 0.9810 - val_loss: 0.1443 - val_acc: 0.9333\n",
      "Epoch 75/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1662 - acc: 0.9238 - val_loss: 0.0685 - val_acc: 1.0000\n",
      "Epoch 76/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1138 - acc: 0.9619 - val_loss: 0.0574 - val_acc: 1.0000\n",
      "Epoch 77/150\n",
      "105/105 [==============================] - 0s 955us/step - loss: 0.1380 - acc: 0.9238 - val_loss: 0.0916 - val_acc: 0.9556\n",
      "Epoch 78/150\n",
      "105/105 [==============================] - 0s 943us/step - loss: 0.1259 - acc: 0.9524 - val_loss: 0.2132 - val_acc: 0.8667\n",
      "Epoch 79/150\n",
      "105/105 [==============================] - 0s 948us/step - loss: 0.1751 - acc: 0.9238 - val_loss: 0.1167 - val_acc: 0.9333\n",
      "Epoch 80/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1248 - acc: 0.9524 - val_loss: 0.0558 - val_acc: 1.0000\n",
      "Epoch 81/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1346 - acc: 0.9619 - val_loss: 0.0997 - val_acc: 0.9556\n",
      "Epoch 82/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1103 - acc: 0.9619 - val_loss: 0.4906 - val_acc: 0.8000\n",
      "Epoch 83/150\n",
      "105/105 [==============================] - 0s 915us/step - loss: 0.1360 - acc: 0.9429 - val_loss: 0.0538 - val_acc: 1.0000\n",
      "Epoch 84/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0998 - acc: 0.9619 - val_loss: 0.1037 - val_acc: 0.9333\n",
      "Epoch 85/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1148 - acc: 0.9619 - val_loss: 0.1665 - val_acc: 0.9333\n",
      "Epoch 86/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1374 - acc: 0.9429 - val_loss: 0.1266 - val_acc: 0.9333\n",
      "Epoch 87/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1726 - acc: 0.9333 - val_loss: 0.0875 - val_acc: 1.0000\n",
      "Epoch 88/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1090 - acc: 0.9714 - val_loss: 0.0982 - val_acc: 0.9556\n",
      "Epoch 89/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1606 - acc: 0.9429 - val_loss: 0.1577 - val_acc: 0.9333\n",
      "Epoch 90/150\n",
      "105/105 [==============================] - 0s 877us/step - loss: 0.1398 - acc: 0.9524 - val_loss: 0.0691 - val_acc: 1.0000\n",
      "Epoch 91/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1331 - acc: 0.9714 - val_loss: 0.1115 - val_acc: 0.9333\n",
      "Epoch 92/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1477 - acc: 0.9238 - val_loss: 0.0668 - val_acc: 0.9778\n",
      "Epoch 93/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1243 - acc: 0.9429 - val_loss: 0.1092 - val_acc: 0.9333\n",
      "Epoch 94/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1158 - acc: 0.9714 - val_loss: 0.1337 - val_acc: 0.9333\n",
      "Epoch 95/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1157 - acc: 0.9333 - val_loss: 0.0891 - val_acc: 0.9556\n",
      "Epoch 96/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1314 - acc: 0.9524 - val_loss: 0.1395 - val_acc: 0.9333\n",
      "Epoch 97/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1284 - acc: 0.9524 - val_loss: 0.0935 - val_acc: 0.9556\n",
      "Epoch 98/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1441 - acc: 0.9524 - val_loss: 0.1217 - val_acc: 0.9556\n",
      "Epoch 99/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1158 - acc: 0.9524 - val_loss: 0.1104 - val_acc: 0.9333\n",
      "Epoch 100/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1171 - acc: 0.9524 - val_loss: 0.1687 - val_acc: 0.9333\n",
      "Epoch 101/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1369 - acc: 0.9619 - val_loss: 0.0625 - val_acc: 1.0000\n",
      "Epoch 102/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1239 - acc: 0.9524 - val_loss: 0.1931 - val_acc: 0.8889\n",
      "Epoch 103/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0862 - acc: 0.9905 - val_loss: 0.0606 - val_acc: 0.9778\n",
      "Epoch 104/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1654 - acc: 0.9238 - val_loss: 0.0980 - val_acc: 0.9333\n",
      "Epoch 105/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1149 - acc: 0.9714 - val_loss: 0.0657 - val_acc: 1.0000\n",
      "Epoch 106/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1170 - acc: 0.9429 - val_loss: 0.0906 - val_acc: 0.9333\n",
      "Epoch 107/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1177 - acc: 0.9333 - val_loss: 0.0444 - val_acc: 0.9778\n",
      "Epoch 108/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1259 - acc: 0.9524 - val_loss: 0.1120 - val_acc: 0.9333\n",
      "Epoch 109/150\n",
      "105/105 [==============================] - 0s 991us/step - loss: 0.1348 - acc: 0.9429 - val_loss: 0.0881 - val_acc: 0.9556\n",
      "Epoch 110/150\n",
      "105/105 [==============================] - 0s 985us/step - loss: 0.1259 - acc: 0.9524 - val_loss: 0.0764 - val_acc: 0.9778\n",
      "Epoch 111/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1161 - acc: 0.9429 - val_loss: 0.1474 - val_acc: 0.9333\n",
      "Epoch 112/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1174 - acc: 0.9714 - val_loss: 0.0625 - val_acc: 0.9778\n",
      "Epoch 113/150\n",
      "105/105 [==============================] - 0s 995us/step - loss: 0.1157 - acc: 0.9429 - val_loss: 0.1794 - val_acc: 0.9111\n",
      "Epoch 114/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1280 - acc: 0.9619 - val_loss: 0.0784 - val_acc: 0.9778\n",
      "Epoch 115/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1335 - acc: 0.9524 - val_loss: 0.0770 - val_acc: 1.0000\n",
      "Epoch 116/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1279 - acc: 0.9524 - val_loss: 0.1685 - val_acc: 0.9333\n",
      "Epoch 117/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1182 - acc: 0.9429 - val_loss: 0.1481 - val_acc: 0.9333\n",
      "Epoch 118/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1262 - acc: 0.9619 - val_loss: 0.0918 - val_acc: 0.9778\n",
      "Epoch 119/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1198 - acc: 0.9429 - val_loss: 0.1663 - val_acc: 0.9333\n",
      "Epoch 120/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1246 - acc: 0.9619 - val_loss: 0.1379 - val_acc: 0.9333\n",
      "Epoch 121/150\n",
      "105/105 [==============================] - 0s 992us/step - loss: 0.1240 - acc: 0.9524 - val_loss: 0.1092 - val_acc: 0.9333\n",
      "Epoch 122/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1747 - acc: 0.9333 - val_loss: 0.2550 - val_acc: 0.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/150\n",
      "105/105 [==============================] - 0s 990us/step - loss: 0.1219 - acc: 0.9714 - val_loss: 0.0700 - val_acc: 0.9778\n",
      "Epoch 124/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1508 - acc: 0.9429 - val_loss: 0.1112 - val_acc: 0.9333\n",
      "Epoch 125/150\n",
      "105/105 [==============================] - 0s 877us/step - loss: 0.1239 - acc: 0.9619 - val_loss: 0.0967 - val_acc: 0.9333\n",
      "Epoch 126/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1552 - acc: 0.9238 - val_loss: 0.0721 - val_acc: 1.0000\n",
      "Epoch 127/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1126 - acc: 0.9619 - val_loss: 0.0731 - val_acc: 0.9778\n",
      "Epoch 128/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1155 - acc: 0.9429 - val_loss: 0.1175 - val_acc: 0.9333\n",
      "Epoch 129/150\n",
      "105/105 [==============================] - 0s 953us/step - loss: 0.1133 - acc: 0.9524 - val_loss: 0.0540 - val_acc: 0.9778\n",
      "Epoch 130/150\n",
      "105/105 [==============================] - 0s 943us/step - loss: 0.1184 - acc: 0.9524 - val_loss: 0.2317 - val_acc: 0.8667\n",
      "Epoch 131/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0813 - acc: 0.9810 - val_loss: 0.0725 - val_acc: 0.9778\n",
      "Epoch 132/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1427 - acc: 0.9429 - val_loss: 0.1739 - val_acc: 0.9333\n",
      "Epoch 133/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1222 - acc: 0.9429 - val_loss: 0.0646 - val_acc: 1.0000\n",
      "Epoch 134/150\n",
      "105/105 [==============================] - 0s 970us/step - loss: 0.1004 - acc: 0.9524 - val_loss: 0.0576 - val_acc: 0.9778\n",
      "Epoch 135/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1395 - acc: 0.9619 - val_loss: 0.0564 - val_acc: 0.9778\n",
      "Epoch 136/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1398 - acc: 0.9619 - val_loss: 0.1281 - val_acc: 0.9333\n",
      "Epoch 137/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0987 - acc: 0.9619 - val_loss: 0.0741 - val_acc: 0.9778\n",
      "Epoch 138/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1226 - acc: 0.9714 - val_loss: 0.0781 - val_acc: 0.9778\n",
      "Epoch 139/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1125 - acc: 0.9429 - val_loss: 0.0650 - val_acc: 0.9778\n",
      "Epoch 140/150\n",
      "105/105 [==============================] - 0s 989us/step - loss: 0.1246 - acc: 0.9429 - val_loss: 0.0825 - val_acc: 0.9556\n",
      "Epoch 141/150\n",
      "105/105 [==============================] - 0s 954us/step - loss: 0.1235 - acc: 0.9524 - val_loss: 0.1411 - val_acc: 0.9111\n",
      "Epoch 142/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1126 - acc: 0.9619 - val_loss: 0.1406 - val_acc: 0.9333\n",
      "Epoch 143/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1071 - acc: 0.9619 - val_loss: 0.1537 - val_acc: 0.9333\n",
      "Epoch 144/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1108 - acc: 0.9524 - val_loss: 0.0711 - val_acc: 0.9778\n",
      "Epoch 145/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1400 - acc: 0.9429 - val_loss: 0.0788 - val_acc: 0.9778\n",
      "Epoch 146/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1161 - acc: 0.9429 - val_loss: 0.0355 - val_acc: 1.0000\n",
      "Epoch 147/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0847 - acc: 0.9810 - val_loss: 0.1692 - val_acc: 0.9333\n",
      "Epoch 148/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1158 - acc: 0.9619 - val_loss: 0.1250 - val_acc: 0.9333\n",
      "Epoch 149/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.0981 - acc: 0.9619 - val_loss: 0.1138 - val_acc: 0.9333\n",
      "Epoch 150/150\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 0.1093 - acc: 0.9619 - val_loss: 0.0988 - val_acc: 0.9333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2387abee7b8>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=150, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "3. R6_InternalLab_AIML_Share_Prices-Eager Execution.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
